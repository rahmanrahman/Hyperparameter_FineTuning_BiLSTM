# -*- coding: utf-8 -*-
"""Salinan dari x fix gf b wv yauda bilstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GO96LexnNcKKgqE7V4UdcUDIkMzhVa_n
"""

# Commented out IPython magic to ensure Python compatibility.
#Import library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import pickle

# %matplotlib inline

# !wget https://raw.githubusercontent.com/ksnugroho/klasifikasi-spam-sms/master/data/dataset_sms_spam_v1.csv

data = pd.read_csv('b2 label gofood.csv', sep=';')
data.head()

indexLabel = data[ (data['label'] == 0)].index
data.drop(indexLabel , inplace=True)
data.head(15)

data.info()

# positif = data[data['label'] == 1]
# negatif = data[data['label'] == 2]
# print(positif.shape)
# print(negatif.shape)

# from sklearn.utils import resample
# positif_downsample = resample(positif,
#              replace=True,
#              n_samples=len(negatif),
#              random_state=42)

# print(positif_downsample.shape)

# data = pd.concat([positif_downsample, negatif])
# print(data["label"].value_counts())

data.head()

"""# Text Preprocessing"""

import re

# Buat fungsi untuk langkah case folding
def casefolding(text):
  text = text.lower()                               # Mengubah teks menjadi lower case
  text = re.sub(r'#([a-zA-Z0-9_]{1,50})','', text)  # menghapus hashtag
  text = re.sub(r'https?://\S+|www\.\S+', '', text) # Menghapus URL
  text = re.sub(r'[-+]?[0-9]+', '', text)           # Menghapus angka
  text = re.sub(r'[^\w\s]','', text)                # Menghapus karakter tanda baca
  text = text.strip()
  return text

raw_sample = data['Tweet'].iloc[5]
case_folding = casefolding(raw_sample)

print('Raw data\t: ', raw_sample)
print('Case folding\t: ', case_folding)

"""### normalisasi"""

!pip -q install sastrawi

import nltk
nltk.download('stopwords')

# Download corpus singkatan
# !wget https://raw.githubusercontent.com/ksnugroho/klasifikasi-spam-sms/master/data/key_norm.csv

key_norm = pd.read_csv('key_norm.csv', sep=';')

def text_normalize(text):
  text = ' '.join([key_norm[key_norm['singkat'] == word]['hasil'].values[0] if (key_norm['singkat'] == word).any() else word for word in text.split()])
  text = str.lower(text)
  return text

"""### filtering"""

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords

stopwords_ind = stopwords.words('indonesian')

len(stopwords_ind)

# Buat fungsi untuk langkah stopword removal

more_stopword = ['rb', 'k', 'in', 'nya', 'ajg', 'anjir', 'anjing']                    # Tambahkan kata dalam daftar stopword
stopwords_ind = stopwords_ind + more_stopword

def remove_stop_words(text):
  clean_words = []
  text = text.split()
  for word in text:
      if word not in stopwords_ind:
          clean_words.append(word)
  return " ".join(clean_words)

raw_sample = data['Tweet'].iloc[17]
case_folding = casefolding(raw_sample)
stopword_removal = remove_stop_words(case_folding)

print('Raw data\t\t: ', raw_sample)
print('Case folding\t\t: ', case_folding)
print('Stopword removal\t: ', stopword_removal)

"""### stemming"""

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

factory = StemmerFactory()
stemmer = factory.create_stemmer()

# Buat fungsi untuk langkah stemming bahasa Indonesia
def stemming(text):
  text = stemmer.stem(text)
  return text

raw_sample = data['Tweet'].iloc[17]
case_folding = casefolding(raw_sample)
stopword_removal = remove_stop_words(case_folding)
text_stemming = stemming(stopword_removal)

print('Raw data\t\t: ', raw_sample)
print('Case folding\t\t: ', case_folding)
print('Stopword removal\t: ', stopword_removal)
print('Stemming\t\t: ', text_stemming)

"""### pipeline"""

# Buat fungsi untuk menggabungkan seluruh langkah text preprocessing
def text_preprocessing_process(text):
  text = casefolding(text)
  text = text_normalize(text)
  text = stemming(text)
  text = remove_stop_words(text)
  return text

# Commented out IPython magic to ensure Python compatibility.
# %%time
# data['clean_teks'] = data['Tweet'].apply(text_preprocessing_process)
# 
# # Perhatikan waktu komputasi ketika proses text preprocessing

data.to_csv('clean_dataset_sms_spam.csv')

"""# Word Embedding

## Word2Vec
"""

#import library
import gensim

# Tokenize kata pada setiap kalimat
def tokenization(text):
  text = re.split('\W+', text)
  return text

sentences = data['clean_teks'].apply(lambda x: tokenization(x.lower()))
sentences

# Defenisikan parameter training Word2Vec

MIN_COUNT = 1       # Mengabaikan semua kata dengan frekuensi total lebih rendah dari ini (opsional)
WINDOW = 5          # Window size. Jarak maksimum antara kata saat ini dan yang diprediksi dalam sebuah kalimat.
EPOCH = 100          # Jumlah iterasi (epoch).
SG = 1              # Algoritma pelatihan: 1 untuk skip-gram, 0 untuk CBOW.
SIZE = 50            # Dimensi vektor
NEGATIVE = 1            # Negative sampling. Jika 0, negative sampling tidak digunakan



# Commented out IPython magic to ensure Python compatibility.
# # Proses training Word2Vec
# 
# %%time
# model_word2vec = gensim.models.Word2Vec(sentences, vector_size=SIZE, sg=SG, min_count=MIN_COUNT, negative=NEGATIVE, window=WINDOW, epochs=EPOCH)

# Save Word2Vec sebagai full model
model_word2vec.save('myvec-word2vec-100.model')

# Save Word2Vec sebagai wordvectors. Hanya menyimpan kata & trained embeddingnya
word2vec_word_vectors = model_word2vec.wv
word2vec_word_vectors.save('myvec-word2vec-100.wordvectors')

# Cari nilai vektor dari kata 'hadiah'
model_word2vec.wv['promo']

# Menemukan kata kata teratas yang paling mirip.
# Menghitung kesamaan dari vektor bobot proyeksi dari kata-kata yang diberikan dan vektor untuk setiap kata dalam model.

model_word2vec.wv.most_similar('promo')

# Karena berdimensi 5, kita membutuhkan
# Kata-kata yang ingin kita plot vektornya
word_list = ['promo', 'ongkir', 'gratis', 'voucher']

# daftar vektor dari kata-kata tersebut
word_vectors = np.array([model_word2vec.wv[w] for w in word_list])
word_vectors

# Reduksi dimensi vektor ke 2D menggunakan PCA
from sklearn.decomposition import PCA, KernelPCA

dimred = PCA(n_components=2, copy=False, whiten=True)
red_word_vectors = dimred.fit_transform(word_vectors)

red_word_vectors

# Implementasi visualisasi dapat di cek pada laman berikut: https://ir.cs.ui.ac.id/alfan/tutorial/gensim_w2v.html

def plot(datas, labels, fc='yellow'):
  # plot the dots
  plt.subplots_adjust(bottom = 0.1)
  plt.scatter(datas[:, 0], datas[:, 1], marker='o')

  # annotate labels
  for label, x, y in zip(labels, datas[:, 0], datas[:, 1]):
    plt.annotate(label, \
                 xy=(x, y), xytext=(-15, 15), \
                 textcoords='offset points', ha='right', va='bottom', \
                 bbox=dict(boxstyle='round,pad=0.3', fc=fc, alpha=0.5), \
                 arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))
  plt.show()

plot(red_word_vectors, word_list)

"""## FastText"""

# !pip -q install fasttext

# Download dan unzip dataset
# Warning: proses download lama dan memakan diskspace yang besar
# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz
# !gunzip cc.en.300.bin.gz

# import fasttext
# ft_model = fasttext.load_model("cc.en.300.bin")

# print(ft_model['hadiah'])

# list_kalimat = ['saya lapar saya tidak makan','saya tidak lapar saya makan']
# word_vectors_fasttext = np.array([ft_model[w] for w in list_kalimat])
# pca = PCA(n_components=2, copy=False, whiten=True)

# fasttext_word_vectors = pca.fit_transform(word_vectors_fasttext)

# plot(word_vectors_fasttext, list_kalimat)

"""# Deep Learning for NLP

## LSTM

### Data Splitting
"""

# Tentukan kolom yang akan digunkan
data = data[['clean_teks', 'label']]
data

data = data.dropna(axis=1)

# Buat data uji, data latih, dan data validasi
from sklearn.model_selection import train_test_split

data_train, data_test = train_test_split(data, test_size=0.2, random_state=5)
data_train, data_val = train_test_split(data_train, test_size=0.25, random_state=5)

# Lihat shape dari masing-masing data
print(data_train.shape)
print(data_val.shape)
print(data_test.shape)

# Pisahkan kolom fitur dan target (label)
trainX = np.array(data_train.iloc[:, 0])
trainY = np.array(data_train.iloc[:, 1])

testX = np.array(data_test.iloc[:, 0])
testY = np.array(data_test.iloc[:, 1])

valX = np.array(data_val.iloc[:, 0])
valY = np.array(data_val.iloc[:, 1])

# One-hot encoding

trainY = pd.get_dummies(trainY).values
testY = pd.get_dummies(testY).values
valY = pd.get_dummies(valY).values

print(trainY.shape)
print(testY.shape)
print(valY.shape)

trainY

"""### Tokenizing Data"""

'''
Proses vektorisasi teks dengan mengubah setiap teks menjadi token urutan bilangan int (setiap bilangan int menjadi indeks token dalam dictionary)
'''
from keras.preprocessing.text import Tokenizer

NUM_WORDS = 100       # Frekuensi kemunculan kata. Hanya num_words-1 yang akan disimpan pada dictionary. Berarti kata dengan kemunculan >=100 tidak digunakan.
OOV_TOKEN = '<unk>'   # Token khusus untuk mengganti kata yang tidak terdaftar dalam dictionary.

tokenizer = Tokenizer(num_words=100, oov_token=OOV_TOKEN, lower=True)
tokenizer.fit_on_texts(trainX)

# Tambahkan token padding untuk menyamakan ukuran dimensi fitur input ke LSTM
tokenizer.word_index['<pad>'] = 0
tokenizer.index_word[0] = '<pad>'

word_index = tokenizer.word_index

# Lihat dictionary yang telah dibuat proses tokenisasi
print(word_index)

len(word_index)

"""### Text to Sequences"""

'''
Proses mengubah token teks menjadi urutan bilangan int.
'''
train_seqs = tokenizer.texts_to_sequences(trainX)
val_seqs = tokenizer.texts_to_sequences(valX)
test_seqs = tokenizer.texts_to_sequences(testX)

# Lihat hasil dari text to sequences
# Setiap bilang int merujuk pada indeks token yang ada dalam dictionary
print(train_seqs)

print(trainX[10])
print(train_seqs[10])

"""### Padding & Truncate"""

'''
Proses mengubah data dari sequence menjadi array bentuk 2D Numpy (num_samples, num_timesteps).
num_timesteps adalah argumen maxlen jika disediakan, atau panjang dari sequence terpanjang dalam daftar.

Sequence yang lebih pendek dari num_timesteps diisi dengan nilai hingga panjangnya sama dengan num_timesteps (padding).
Sequence yang lebih panjang dari num_timesteps dipotong sehingga sesuai dengan panjang yang diinginkan (truncate).
'''

from tensorflow.keras.preprocessing.sequence import pad_sequences

# Lihat distribusi jumlah kata dalam X_train_seq.
# Pada contoh ini, max_len tidak ditentukan, sehingga semua data dari sequence akan di padding menjad panjang sesuai dengan sequnce terpanjang
l = [len(i) for i in train_seqs]
l = np.array(l)

print('minimum number of words: {}'.format(l.min()))
print('median number of words: {}'.format(np.median(l)))
print('average number of words: {}'.format(l.mean()))
print('maximum number of words: {}'.format(l.max()))

PADDING = 'pre' # Terdapat dua pilihan dalam melakukan padding 'pre' or 'post'.

train_seqs = pad_sequences(train_seqs, padding=PADDING)
val_seqs = pad_sequences(val_seqs, padding=PADDING)
test_seqs = pad_sequences(test_seqs, padding=PADDING)

print(trainX[10])
print(train_seqs[10])

"""### Define Model"""

from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, Flatten

model_LSTM = Sequential()
model_LSTM.add(Embedding(len(word_index), 200))
model_LSTM.add(LSTM(100))
model_LSTM.add(Dense(2, activation='softmax'))

model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

"""### Training"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# EPOCH = 10
# BATCH_SIZE = 32
# 
# history_LSTM = model_LSTM.fit(train_seqs, trainY, epochs=EPOCH, batch_size=BATCH_SIZE, validation_data=(val_seqs, valY))

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

plot_graphs(history_LSTM, 'accuracy')
plot_graphs(history_LSTM, 'loss')

print('\nEpoch No.  Train Accuracy  Train Loss      Val Accuracy    Val Loss')
for i in range(EPOCH):
  print('{:8d} {:10f} \t {:10f} \t {:10f} \t {:10f}'.format(i + 1, history_LSTM.history['accuracy'][i], history_LSTM.history['loss'][i], history_LSTM.history['val_accuracy'][i], history_LSTM.history['val_loss'][i]))

"""### Evaluate"""

# Lakukan prediksi pada data uji
y_pred = np.argmax(model_LSTM.predict(test_seqs), axis=1)
y_true = np.argmax(testY, axis=1)

loss, accuracy = model_LSTM.evaluate(test_seqs, testY)

from sklearn.metrics import classification_report

print(classification_report(y_pred, y_true))

"""## BiLSTM + Word Embedding"""

# Load word2vec yang telah kita simpan
from gensim.models import KeyedVectors

my_word2vec = KeyedVectors.load('myvec-word2vec-100.wordvectors', mmap='r')
my_word2vec_vocab = my_word2vec.key_to_index

VOCAB_SIZE = len(my_word2vec_vocab)
EMBEDDING_SIZE = my_word2vec.vector_size

print(VOCAB_SIZE)
print(EMBEDDING_SIZE)

word2vec_dict = {}

for word in my_word2vec_vocab:
    word2vec_dict[word] = my_word2vec.get_vector(word)

print('The numbers of key-value pairs: ', len(word2vec_dict)) # Harus sama dengan ukuran vocab word2vec

"""### Token to Embedding"""

EMBEDDING_MATRIX = np.zeros(shape=(len(word_index), EMBEDDING_SIZE))

for word, i in tokenizer.word_index.items():
  embed_vector = word2vec_dict.get(word)
  if embed_vector is not None:               # Word is in the vocabulary learned by the w2v model
    EMBEDDING_MATRIX[i] = embed_vector

"""### Define Model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout
from tensorflow.keras.initializers import Constant

model_BiLSTM_w2v = Sequential()
model_BiLSTM_w2v.add(Embedding(len(word_index), 50, embeddings_initializer = Constant(EMBEDDING_MATRIX)))
model_BiLSTM_w2v.add(Bidirectional(LSTM(64)))
model_BiLSTM_w2v.add(Dropout(0.2))
model_BiLSTM_w2v.add(Dense(2, activation='softmax'))

# Compile model
model_BiLSTM_w2v.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# EPOCH = 75
# BATCH_SIZE = 32
# 
# history_BiLSTM_w2v = model_BiLSTM_w2v.fit(train_seqs, trainY, epochs=EPOCH, batch_size=BATCH_SIZE, validation_data=(val_seqs, valY))

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

plot_graphs(history_BiLSTM_w2v, 'accuracy')
plot_graphs(history_BiLSTM_w2v, 'loss')

print('\nEpoch No.  Train Accuracy  Train Loss      Val Accuracy    Val Loss')
for i in range(EPOCH):
  print('{:8d} {:10f} \t {:10f} \t {:10f} \t {:10f}'.format(i + 1, history_BiLSTM_w2v.history['accuracy'][i], history_BiLSTM_w2v.history['loss'][i], history_BiLSTM_w2v.history['val_accuracy'][i], history_BiLSTM_w2v.history['val_loss'][i]))

# Lakukan prediksi pada data uji
y_pred = np.argmax(model_BiLSTM_w2v.predict(test_seqs), axis=1)
y_true = np.argmax(testY, axis=1)

loss, accuracy = model_BiLSTM_w2v.evaluate(test_seqs, testY)

from sklearn.metrics import classification_report

print(classification_report(y_pred, y_true))

# trainY

# max_len = trainY.shape[0]

# ann = Sequential()
# ann.add(Dense(32, activation="elu", input_shape=(max_len)))
# ann.add(Dense(1024, activation="elu"))
# ann.add(Dense(512, activation="elu"))
# ann.add(Dense(256, activation="elu"))
# ann.add(Dense(128, activation="elu"))
# ann.add(Dense(16))
# ann.add(Activation("sigmoid"))

"""## Fine Tuning"""



# Tentukan nilai hyperparamter untuk fine-tuning
EPOCHS = 50
BATCH_SIZE = 64
LEARNING_RATE = 5e-20

# Commented out IPython magic to ensure Python compatibility.
# %%time
# bilstm_tuning = model_BiLSTM_w2v.fit(train_seqs, trainY, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(val_seqs, valY))

# Buat fungsi untuk plotting hasil training
def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel('Epochs')
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

plot_graphs(bilstm_tuning, 'accuracy')
plot_graphs(bilstm_tuning, 'loss')

print('\nEpoch No.  Train Accuracy  Train Loss      Val Accuracy    Val Loss')
for i in range(EPOCHS):
  print('{:8d} {:10f} \t {:10f} \t {:10f} \t {:10f}'.format(i + 1, bilstm_tuning.history['accuracy'][i], bilstm_tuning.history['loss'][i], bilstm_tuning.history['val_accuracy'][i], bilstm_tuning.history['val_loss'][i]))

score = model_BiLSTM_w2v.evaluate(val_seqs, valY)

print("Test Accuracy:", score[1])

from sklearn.metrics import classification_report

print(classification_report(y_pred, y_true))

pickle.dump(bilstm_tuning,open("bilstm.pkl","wb"))

model_BiLSTM_w2v.save_weights('bert-model.h5')

